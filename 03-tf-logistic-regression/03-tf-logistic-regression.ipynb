{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow2: Image Classification using Logistic Regression\n",
    "\n",
    "### This notebook is a tensorflow port of https://jovian.ml/aakashns/03-logistic-regression\n",
    "\n",
    "Despite the structural differences in Tensorflow and PyTorch, I have tried to port the torch notebooks to tensorflow, which helps in learning both frameworks along with the course [PyTorch: Zero to GANs](https://jovian.ml/aakashns/01-pytorch-basics) by Aakash\n",
    "\n",
    "#### Part 3 of \"Tensorflow: Zero toÂ GANs\"\n",
    "\n",
    "*This post is the third in a series of tutorials on building deep learning models with tensorflow, an open source neural networks library. Check out the full series:*\n",
    "\n",
    "1. [Tensorflow Basics: Tensors & Gradients](https://jovian.ml/kartik.godawat/01-tensorflow-basics)\n",
    "2. [Linear Regression & Gradient Descent](https://jovian.ml/kartik.godawat/02-tf-linear-regression)\n",
    "3. [Image Classfication using Logistic Regression](https://jovian.ml/kartik.godawat/03-tf-logistic-regression) \n",
    "4. [Training Deep Neural Networks on a GPU](https://jovian.ml/kartik.godawat/04-tf-feedforward-nn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll use our existing knowledge of PyTorch and linear regression to solve a very different kind of problem: *image classification*. We'll use the famous [*MNIST Handwritten Digits Database*](http://yann.lecun.com/exdb/mnist/) as our training dataset. It consists of 28px by 28px grayscale images of handwritten digits (0 to 9), along with labels for each image indicating which digit it represents. Here are some sample images from the dataset:\n",
    "\n",
    "![mnist-sample](https://i.imgur.com/CAYnuo1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset import and exploration\n",
    "Tensorflow2.x as of now has two ways of importing mnist dataset using `tf.keras.datasets` and using a library called `tensorflow_datasets`. Let's go through both approaches now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach-1: keras.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`help()` function can list the available functions in a module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package tensorflow_core.keras.datasets in tensorflow_core.keras:\n",
      "\n",
      "NAME\n",
      "    tensorflow_core.keras.datasets - Public API for tf.keras.datasets namespace.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    boston_housing (package)\n",
      "    cifar10 (package)\n",
      "    cifar100 (package)\n",
      "    fashion_mnist (package)\n",
      "    imdb (package)\n",
      "    mnist (package)\n",
      "    reuters (package)\n",
      "\n",
      "FILE\n",
      "    /home/d/envs/tftwo/lib/python3.6/site-packages/tensorflow_core/python/keras/api/_v2/keras/datasets/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.keras.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is already split into a train and a test set, 60k in train and 10k in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is available to us as numpy array. In order to view the images of 28x28 size, we can use matplotlib to plot the data as an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with importing `matplotlib`, a special statement `%matplotlib inline` is added to indicate to Jupyter that we want to plot the graphs within the notebook. Without this line, Jupyter will show the image in a popup. Statements starting with `%` are called IPython magic commands, and are used to configure the behavior of Jupyter itself. You can find a full list of magic commands here: https://ipython.readthedocs.io/en/stable/interactive/magics.html .\n",
    "\n",
    "Let's look at a couple of images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0], cmap='gray')\n",
    "print(\"Label:\", y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0,10:15,10:15], cmap='gray')\n",
    "print(\"Label:\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's evident that these images are quite small in size, and recognizing the digits can sometimes be hard even for the human eye. While it's useful to look at these images, there's just one problem here: Tensorflow doesn't know how to work with images. We need to convert the images into tensors. We can do this by specifying a transform while creating our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((28, 28), ()), types: (tf.uint8, tf.uint8)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar to the dataset we created in https://jovian.ml/kartik.godawat/02-tf-linear-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach-2: tensorflow_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tensorflow_datasets` is another official library by tensorflow team which helps loading different datasets directly as `tf.data.Dataset`. Keras datasets are very basic toy datasets, whereas this libary is suitable for large datasets as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\r\n",
      "You should consider upgrading via the '/home/d/envs/tftwo/bin/python3.6 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of available datasets in the current libary can also be viewed from code like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract_reasoning',\n",
       " 'aeslc',\n",
       " 'aflw2k3d',\n",
       " 'amazon_us_reviews',\n",
       " 'arc',\n",
       " 'bair_robot_pushing_small',\n",
       " 'beans',\n",
       " 'big_patent',\n",
       " 'bigearthnet',\n",
       " 'billsum',\n",
       " 'binarized_mnist',\n",
       " 'binary_alpha_digits',\n",
       " 'blimp',\n",
       " 'c4',\n",
       " 'caltech101',\n",
       " 'caltech_birds2010',\n",
       " 'caltech_birds2011',\n",
       " 'cars196',\n",
       " 'cassava',\n",
       " 'cats_vs_dogs',\n",
       " 'celeb_a',\n",
       " 'celeb_a_hq',\n",
       " 'cfq',\n",
       " 'chexpert',\n",
       " 'cifar10',\n",
       " 'cifar100',\n",
       " 'cifar10_1',\n",
       " 'cifar10_corrupted',\n",
       " 'citrus_leaves',\n",
       " 'cityscapes',\n",
       " 'civil_comments',\n",
       " 'clevr',\n",
       " 'cmaterdb',\n",
       " 'cnn_dailymail',\n",
       " 'coco',\n",
       " 'coil100',\n",
       " 'colorectal_histology',\n",
       " 'colorectal_histology_large',\n",
       " 'common_voice',\n",
       " 'cos_e',\n",
       " 'crema_d',\n",
       " 'curated_breast_imaging_ddsm',\n",
       " 'cycle_gan',\n",
       " 'deep_weeds',\n",
       " 'definite_pronoun_resolution',\n",
       " 'dementiabank',\n",
       " 'diabetic_retinopathy_detection',\n",
       " 'div2k',\n",
       " 'dmlab',\n",
       " 'downsampled_imagenet',\n",
       " 'dsprites',\n",
       " 'dtd',\n",
       " 'duke_ultrasound',\n",
       " 'emnist',\n",
       " 'eraser_multi_rc',\n",
       " 'esnli',\n",
       " 'eurosat',\n",
       " 'fashion_mnist',\n",
       " 'flic',\n",
       " 'flores',\n",
       " 'food101',\n",
       " 'forest_fires',\n",
       " 'gap',\n",
       " 'geirhos_conflict_stimuli',\n",
       " 'german_credit_numeric',\n",
       " 'gigaword',\n",
       " 'glue',\n",
       " 'groove',\n",
       " 'higgs',\n",
       " 'horses_or_humans',\n",
       " 'i_naturalist2017',\n",
       " 'image_label_folder',\n",
       " 'imagenet2012',\n",
       " 'imagenet2012_corrupted',\n",
       " 'imagenet2012_subset',\n",
       " 'imagenet_resized',\n",
       " 'imagenette',\n",
       " 'imagewang',\n",
       " 'imdb_reviews',\n",
       " 'iris',\n",
       " 'kitti',\n",
       " 'kmnist',\n",
       " 'lfw',\n",
       " 'librispeech',\n",
       " 'librispeech_lm',\n",
       " 'libritts',\n",
       " 'ljspeech',\n",
       " 'lm1b',\n",
       " 'lost_and_found',\n",
       " 'lsun',\n",
       " 'malaria',\n",
       " 'math_dataset',\n",
       " 'mnist',\n",
       " 'mnist_corrupted',\n",
       " 'movie_rationales',\n",
       " 'moving_mnist',\n",
       " 'multi_news',\n",
       " 'multi_nli',\n",
       " 'multi_nli_mismatch',\n",
       " 'natural_questions',\n",
       " 'newsroom',\n",
       " 'nsynth',\n",
       " 'omniglot',\n",
       " 'open_images_challenge2019_detection',\n",
       " 'open_images_v4',\n",
       " 'opinosis',\n",
       " 'oxford_flowers102',\n",
       " 'oxford_iiit_pet',\n",
       " 'para_crawl',\n",
       " 'patch_camelyon',\n",
       " 'pet_finder',\n",
       " 'places365_small',\n",
       " 'plant_leaves',\n",
       " 'plant_village',\n",
       " 'plantae_k',\n",
       " 'qa4mre',\n",
       " 'quickdraw_bitmap',\n",
       " 'reddit',\n",
       " 'reddit_tifu',\n",
       " 'resisc45',\n",
       " 'robonet',\n",
       " 'rock_paper_scissors',\n",
       " 'rock_you',\n",
       " 'samsum',\n",
       " 'savee',\n",
       " 'scan',\n",
       " 'scene_parse150',\n",
       " 'scicite',\n",
       " 'scientific_papers',\n",
       " 'shapes3d',\n",
       " 'smallnorb',\n",
       " 'snli',\n",
       " 'so2sat',\n",
       " 'speech_commands',\n",
       " 'squad',\n",
       " 'stanford_dogs',\n",
       " 'stanford_online_products',\n",
       " 'starcraft_video',\n",
       " 'stl10',\n",
       " 'sun397',\n",
       " 'super_glue',\n",
       " 'svhn_cropped',\n",
       " 'ted_hrlr_translate',\n",
       " 'ted_multi_translate',\n",
       " 'tedlium',\n",
       " 'tf_flowers',\n",
       " 'the300w_lp',\n",
       " 'tiny_shakespeare',\n",
       " 'titanic',\n",
       " 'trivia_qa',\n",
       " 'uc_merced',\n",
       " 'ucf101',\n",
       " 'vgg_face2',\n",
       " 'visual_domain_decathlon',\n",
       " 'voc',\n",
       " 'voxceleb',\n",
       " 'waymo_open_dataset',\n",
       " 'web_questions',\n",
       " 'wider_face',\n",
       " 'wiki40b',\n",
       " 'wikihow',\n",
       " 'wikipedia',\n",
       " 'wmt14_translate',\n",
       " 'wmt15_translate',\n",
       " 'wmt16_translate',\n",
       " 'wmt17_translate',\n",
       " 'wmt18_translate',\n",
       " 'wmt19_translate',\n",
       " 'wmt_t2t_translate',\n",
       " 'wmt_translate',\n",
       " 'xnli',\n",
       " 'xsum',\n",
       " 'yelp_polarity_reviews']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.list_builders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to keras `mnist.load_data()`, `tfds.load()` api can be used to load mnist data. We're passing `as_supervised=True` for the load method to return a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The info regarding versioning and other metadata can be viewed in the info object we recieved by passing `with_info=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='mnist',\n",
       "    version=3.0.1,\n",
       "    description='The MNIST database of handwritten digits.',\n",
       "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
       "    }),\n",
       "    total_num_examples=70000,\n",
       "    splits={\n",
       "        'test': 10000,\n",
       "        'train': 60000,\n",
       "    },\n",
       "    supervised_keys=('image', 'label'),\n",
       "    citation=\"\"\"@article{lecun2010mnist,\n",
       "      title={MNIST handwritten digit database},\n",
       "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "      journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n",
       "      volume={2},\n",
       "      year={2010}\n",
       "    }\"\"\",\n",
       "    redistribution_info=,\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have gone ahead with any of the data loads for mnist data. Here, I am choosing to go forward with tensorflow_datasets approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Datasets\n",
    "\n",
    "While building real world machine learning models, it is quite common to split the dataset into 3 parts:\n",
    "\n",
    "1. **Training set** - used to train the model i.e. compute the loss and adjust the weights of the model using gradient descent.\n",
    "2. **Validation set** - used to evaluate the model while training, adjust hyperparameters (learning rate etc.) and pick the best version of the model.\n",
    "3. **Test set** - used to compare different models, or different types of modeling approaches, and report the final accuracy of the model.\n",
    "\n",
    "In the MNIST dataset, there are 60,000 training images, and 10,000 test images. The test set is standardized so that different researchers can report the results of their models against the same set of images. \n",
    "\n",
    "Since there's no predefined validation set, we must manually split the 60,000 images into training and validation datasets. Let's set aside 10,000 randomly chosen images for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.shuffle(buffer_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we shuffle the data, then we use `take()` and `skip()` methods to split the data into train and validation datasets. `buffer_size` in shuffle is to restrict loading the entire data into memory and perform a shuffle. This is necessary for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds.take(50000)\n",
    "val_ds = ds.skip(50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, `tf.data.Dataset` at the time of writing does not support exposing count as a paramater. In order to get the count, there are some hacks, worst of which is below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(train_ds)), len(list(val_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for larger datasets, loading entire dataset into memory isn't possible, so while this works for small datasets, it's not a good practice to get the counts like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like [before](https://jovian.ml/kartik.godawat/02-tf-linear-regression/v/3#C51), instead of loading all data, we create a batch to load only 128 images into memory. We do this for both training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "val_ds = val_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be iterated upon and will return data in batches of 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 28, 28, 1)\n",
      "tf.Tensor(\n",
      "[[[0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]]\n",
      "\n",
      " [[0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]]\n",
      "\n",
      " [[0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]]\n",
      "\n",
      " [[0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]]\n",
      "\n",
      " [[0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]]], shape=(5, 5, 1), dtype=uint8)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_ds:\n",
    "    print(x.shape)\n",
    "    print(x[0,10:15,10:15])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the values here are between 0-255 representing a grayscale image. We will deal with it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Now that we have prepared our data loaders, we can define our model.\n",
    "\n",
    "* A **logistic regression** model is almost identical to a linear regression model i.e. there are weights and bias matrices, and the output is obtained using simple matrix operations (`pred = x @ w.t() + b`). \n",
    "\n",
    "* Just as we did with [linear regression](https://jovian.ml/kartik.godawat/02-tf-linear-regression), we can use `Sequential` to create the model instead of defining and initializing the matrices manually.\n",
    "\n",
    "* The output for each image is vector of size 10, with each element of the vector signifying the probability a particular target label (i.e. 0 to 9). The predicted label for an image is simply the one with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "model = Sequential([\n",
    "  Dense(num_classes, input_shape=(28*28,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, this model is a lot larger than our previous model, in terms of the number of parameters. Let's take a look at the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense/kernel:0' shape=(784, 10) dtype=float32, numpy=\n",
       " array([[-0.02152888,  0.05879454,  0.00226832, ...,  0.0072439 ,\n",
       "         -0.08685475, -0.01775543],\n",
       "        [-0.07275616, -0.03554178, -0.06286011, ..., -0.0650673 ,\n",
       "          0.05680438,  0.01267791],\n",
       "        [ 0.02990201,  0.04954509, -0.05144789, ...,  0.04712003,\n",
       "          0.0005519 , -0.06900601],\n",
       "        ...,\n",
       "        [-0.02774297,  0.08260164, -0.07678004, ..., -0.01186107,\n",
       "         -0.08320007, -0.02955337],\n",
       "        [ 0.04106381, -0.01671977,  0.08499026, ...,  0.03367276,\n",
       "         -0.05307632,  0.05747337],\n",
       "        [ 0.055009  ,  0.02731592, -0.05412671, ...,  0.00771543,\n",
       "         -0.02146795, -0.07764859]], dtype=float32)>,\n",
       " <tf.Variable 'dense/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are a total of 7850 parameters here, conceptually nothing has changed so far. Let's try and generate some outputs using our model. We'll take the first batch of 100 images from our dataset, and pass them into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[3 9 4 2 4 8 6 7 3 6 1 1 1 8 4 7 2 8 3 4 1 7 4 8 5 5 6 4 4 6 3 3 4 7 7 8 9\n",
      " 4 9 4 9 1 6 4 6 1 0 9 7 6 9 9 9 7 4 6 8 8 4 5 2 3 8 9 2 9 6 1 6 1 7 1 2 4\n",
      " 9 5 5 5 4 9 7 2 4 1 2 9 6 4 6 1 1 9 4 7 9 6 3 8 3 4 1 4 2 9 1 5 2 8 9 1 2\n",
      " 8 6 8 3 9 2 2 1 6 7 7 7 8 4 7 5 3], shape=(128,), dtype=int64)\n",
      "(128, 28, 28, 1)\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"dense_input:0\", shape=(None, 784), dtype=float32) for input (None, 784), but it was re-called on a Tensor with incompatible shape (128, 28, 28, 1).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"dense_input:0\", shape=(None, 784), dtype=float32) for input (None, 784), but it was re-called on a Tensor with incompatible shape (128, 28, 28, 1).\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Could not find valid device for node.\nNode:{{node MatMul}}\nAll kernels registered for op MatMul :\n  device='XLA_GPU'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]\n  device='XLA_CPU'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]\n  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]\n  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]\n  device='CPU'; label='eigen'; T in [DT_COMPLEX128]\n  device='CPU'; label='eigen'; T in [DT_COMPLEX64]\n  device='CPU'; label='eigen'; T in [DT_INT64]\n  device='CPU'; label='eigen'; T in [DT_INT32]\n  device='CPU'; label='eigen'; T in [DT_BFLOAT16]\n  device='CPU'; label='eigen'; T in [DT_HALF]\n  device='CPU'; label='eigen'; T in [DT_DOUBLE]\n  device='CPU'; label='eigen'; T in [DT_FLOAT]\n  device='CPU'; T in [DT_COMPLEX128]\n  device='CPU'; T in [DT_COMPLEX64]\n  device='CPU'; T in [DT_INT64]\n  device='CPU'; T in [DT_INT32]\n  device='CPU'; T in [DT_BFLOAT16]\n  device='CPU'; T in [DT_HALF]\n  device='CPU'; T in [DT_DOUBLE]\n  device='CPU'; T in [DT_FLOAT]\n  device='GPU'; label='cublas'; T in [DT_HALF]\n  device='GPU'; label='cublas'; T in [DT_COMPLEX128]\n  device='GPU'; label='cublas'; T in [DT_COMPLEX64]\n  device='GPU'; label='cublas'; T in [DT_DOUBLE]\n  device='GPU'; label='cublas'; T in [DT_FLOAT]\n  device='GPU'; T in [DT_HALF]\n  device='GPU'; T in [DT_COMPLEX128]\n  device='GPU'; T in [DT_COMPLEX64]\n  device='GPU'; T in [DT_DOUBLE]\n  device='GPU'; T in [DT_FLOAT]\n [Op:MatMul] name: sequential/dense/Tensordot/MatMul/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-92c0273d59c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tftwo/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tftwo/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[0;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tftwo/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    715\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m    716\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tftwo/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tftwo/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tftwo/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0;31m# Broadcasting is required for the inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandard_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m       \u001b[0;31m# Reshape the output back to the original ndim of the input.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tftwo/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mtensordot\u001b[0;34m(a, b, axes, name)\u001b[0m\n\u001b[1;32m   4104\u001b[0m     b_reshape, b_free_dims, b_free_dims_static = _tensordot_reshape(\n\u001b[1;32m   4105\u001b[0m         b, b_axes, True)\n\u001b[0;32m-> 4106\u001b[0;31m     \u001b[0mab_matmul\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_reshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_reshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_free_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_free_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4108\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mab_matmul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_free_dims\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_free_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tftwo/lib/python3.6/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tftwo/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   2796\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2797\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[0;32m-> 2798\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   2799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tftwo/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5614\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5615\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5616\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5617\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5618\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtranspose_a\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tftwo/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6604\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6605\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6606\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6607\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tftwo/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Could not find valid device for node.\nNode:{{node MatMul}}\nAll kernels registered for op MatMul :\n  device='XLA_GPU'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]\n  device='XLA_CPU'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]\n  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]\n  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]\n  device='CPU'; label='eigen'; T in [DT_COMPLEX128]\n  device='CPU'; label='eigen'; T in [DT_COMPLEX64]\n  device='CPU'; label='eigen'; T in [DT_INT64]\n  device='CPU'; label='eigen'; T in [DT_INT32]\n  device='CPU'; label='eigen'; T in [DT_BFLOAT16]\n  device='CPU'; label='eigen'; T in [DT_HALF]\n  device='CPU'; label='eigen'; T in [DT_DOUBLE]\n  device='CPU'; label='eigen'; T in [DT_FLOAT]\n  device='CPU'; T in [DT_COMPLEX128]\n  device='CPU'; T in [DT_COMPLEX64]\n  device='CPU'; T in [DT_INT64]\n  device='CPU'; T in [DT_INT32]\n  device='CPU'; T in [DT_BFLOAT16]\n  device='CPU'; T in [DT_HALF]\n  device='CPU'; T in [DT_DOUBLE]\n  device='CPU'; T in [DT_FLOAT]\n  device='GPU'; label='cublas'; T in [DT_HALF]\n  device='GPU'; label='cublas'; T in [DT_COMPLEX128]\n  device='GPU'; label='cublas'; T in [DT_COMPLEX64]\n  device='GPU'; label='cublas'; T in [DT_DOUBLE]\n  device='GPU'; label='cublas'; T in [DT_FLOAT]\n  device='GPU'; T in [DT_HALF]\n  device='GPU'; T in [DT_COMPLEX128]\n  device='GPU'; T in [DT_COMPLEX64]\n  device='GPU'; T in [DT_DOUBLE]\n  device='GPU'; T in [DT_FLOAT]\n [Op:MatMul] name: sequential/dense/Tensordot/MatMul/"
     ]
    }
   ],
   "source": [
    "for images, labels in train_ds:\n",
    "    print(labels)\n",
    "    print(images.shape)\n",
    "    outputs = model(images)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads to an error, because our input data does not have the right shape. Our images are of the shape 28x28, but we need them to be vectors of size 784 i.e. we need to flatten them out.\n",
    "\n",
    "We have two approaches here to flatten the data.\n",
    "- add a Reshape Layer\n",
    "- modify the dataset and flatten it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach-1: tensorflow.keras.layers.Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name suggests, the objective of the layer `tensorflow.keras.layers.Reshape` is to reshape the input_shape into a given output_shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Reshape((28*28,), input_shape=(28,28,1)),\n",
    "    Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that it's working now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[4 5 5 4 7 4 3 1 3 7 8 1 1 1 3 9 0 4 6 2 3 4 6 5 9 2 8 1 8 4 4 3 9 4 7 3 3\n",
      " 4 1 4 9 7 4 5 7 1 5 7 0 4 2 4 5 7 8 7 2 6 9 8 6 0 6 1 7 6 9 9 7 2 2 6 4 5\n",
      " 0 3 9 8 8 1 7 9 4 7 8 9 8 2 9 8 1 5 8 4 3 9 1 2 6 1 1 9 4 8 4 3 9 9 8 4 9\n",
      " 6 0 2 4 9 4 9 9 1 5 6 1 2 9 7 8 8], shape=(128,), dtype=int64)\n",
      "(128, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_ds:\n",
    "    print(labels)\n",
    "    print(images.shape)\n",
    "    outputs = model(images)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach-2: tf.data.Dataset.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we keep the model as same, but instead we modify our train and val datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  Dense(num_classes, input_shape=(28*28,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to `map` each row in the dataset to a function and apply a transformation.\n",
    "- Since we've already converted the dataset into a batch dataset, we need to `unbatch` it first before using `map`\n",
    "- Since our dataset was in range of 0-255, it's usually a good idea to get the dataset in a range of 0-1. So we normalize it by dividing it by 255.\n",
    "- `tf.reshape()` then reshapes the tensor into a flattened shape of [28*28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapp(x):\n",
    "    x = x / 255\n",
    "    return tf.reshape(x, [28*28])\n",
    "train_ds = train_ds.unbatch().map(lambda x,y: (mapp(x),y)).batch(batch_size)\n",
    "val_ds = val_ds.unbatch().map(lambda x,y: (mapp(x),y)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that it's working now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.shape :  (128, 10)\n",
      "Sample outputs :\n",
      " tf.Tensor(\n",
      "[[ 0.6407453   0.11143446  0.09817434  0.55807185 -0.39174902 -0.11454432\n",
      "   0.3625155   0.15150255 -0.08574978  0.02495538]\n",
      " [-0.47593075 -0.3733762   0.31717196  0.88274086  0.18745703  0.2640057\n",
      "   0.5768487   0.35867417 -0.61051834 -0.45194724]], shape=(2, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_ds:\n",
    "    outputs = model(images)\n",
    "    break\n",
    "print('outputs.shape : ', outputs.shape)\n",
    "print('Sample outputs :\\n', outputs[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the 100 input images, we get 10 outputs, one for each class. As discussed earlier, we'd like these outputs to represent probabilities, but for that the elements of each output row must lie between 0 to 1 and add up to 1, which is clearly not the case here. \n",
    "\n",
    "To convert the output rows into probabilities, we use the softmax function, which has the following formula:\n",
    "\n",
    "![softmax](https://i.imgur.com/EAh9jLN.png)\n",
    "\n",
    "First we replace each element `yi` in an output row by `e^yi`, which makes all the elements positive, and then we divide each element by the sum of all elements to ensure that they add up to 1. \n",
    "\n",
    "While it's easy to implement the softmax function (you should try it!), we'll use the implementation that's provided within tensorflow, because it works well with multidimensional tensors (a list of output rows in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample probabilities:\n",
      " tf.Tensor(\n",
      "[[0.15845512 0.09333178 0.09210235 0.14588198 0.05642866 0.07445402\n",
      "  0.11996999 0.09714732 0.07662906 0.08559968]\n",
      " [0.05177516 0.05736675 0.1144357  0.20145823 0.10051412 0.10851049\n",
      "  0.14836714 0.11928497 0.04525544 0.05303192]], shape=(2, 10), dtype=float32)\n",
      "Sum:  tf.Tensor(0.9999999, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Apply softmax for each output row\n",
    "probs = tf.nn.softmax(outputs)\n",
    "\n",
    "# Look at sample probabilities\n",
    "print(\"Sample probabilities:\\n\", probs[:2])\n",
    "\n",
    "# Add up the probabilities of an output row\n",
    "print(\"Sum: \", tf.reduce_sum(probs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can determine the predicted label for each image by simply choosing the index of the element with the highest probability in each output row. This is done using `tf.argmax`, which returns the index of the largest element along a particular dimension of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
       "array([0, 3, 3, 6, 3, 3, 3, 6, 3, 7, 3, 3, 7, 3, 6, 3, 3, 7, 4, 0, 3, 3,\n",
       "       3, 3, 0, 7, 2, 3, 7, 3, 7, 3, 3, 6, 7, 3, 3, 6, 8, 6, 3, 1, 3, 3,\n",
       "       6, 7, 6, 6, 3, 5, 8, 3, 3, 7, 6, 3, 3, 1, 6, 8, 1, 3, 6, 6, 0, 3,\n",
       "       6, 3, 3, 3, 4, 3, 3, 3, 3, 3, 7, 5, 6, 4, 6, 3, 4, 4, 3, 6, 3, 3,\n",
       "       3, 7, 7, 6, 6, 3, 3, 6, 6, 6, 3, 6, 3, 2, 7, 5, 3, 6, 6, 7, 3, 7,\n",
       "       3, 5, 0, 6, 6, 3, 6, 6, 7, 6, 3, 7, 7, 8, 3, 3, 3, 2])>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `tf.reduce_max`, which returns the largest element along a particular dimension of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       "array([0.15845512, 0.20145823, 0.1473594 , 0.18312174, 0.14684515,\n",
       "       0.13821913, 0.20344476, 0.21454631, 0.17464176, 0.17615716,\n",
       "       0.19924934, 0.191635  , 0.15574196, 0.15451096, 0.16498904,\n",
       "       0.1951823 , 0.14699866, 0.19517234, 0.13819592, 0.15195793,\n",
       "       0.16975632, 0.16272886, 0.22997463, 0.22378232, 0.1862738 ,\n",
       "       0.16766134, 0.16471116, 0.1702503 , 0.16441748, 0.26557878,\n",
       "       0.1537855 , 0.22680064, 0.15283428, 0.1730881 , 0.21709965,\n",
       "       0.16072929, 0.25171223, 0.21650884, 0.17280476, 0.15892385,\n",
       "       0.18534954, 0.25566936, 0.18228438, 0.28055292, 0.22572468,\n",
       "       0.23617677, 0.1424967 , 0.16862112, 0.1510295 , 0.1263147 ,\n",
       "       0.1499951 , 0.17324796, 0.20319031, 0.22055261, 0.126317  ,\n",
       "       0.16002771, 0.1813619 , 0.22873938, 0.14520727, 0.18482171,\n",
       "       0.20929377, 0.2025224 , 0.14242812, 0.15669467, 0.13586389,\n",
       "       0.1789451 , 0.16269937, 0.2045537 , 0.1739732 , 0.17395262,\n",
       "       0.14301243, 0.14859805, 0.2808446 , 0.2413652 , 0.18902478,\n",
       "       0.15055   , 0.19494657, 0.14827704, 0.29900265, 0.20721443,\n",
       "       0.19855374, 0.14879383, 0.20562245, 0.21359439, 0.2426899 ,\n",
       "       0.13866879, 0.18640742, 0.15676235, 0.22842705, 0.14787412,\n",
       "       0.21108647, 0.17740494, 0.12390447, 0.1785766 , 0.27527842,\n",
       "       0.13843395, 0.22527412, 0.18204387, 0.24970983, 0.15001714,\n",
       "       0.16735183, 0.1597067 , 0.14055347, 0.17687747, 0.19086452,\n",
       "       0.21770997, 0.2036499 , 0.14359425, 0.16029498, 0.26973426,\n",
       "       0.20663755, 0.12652694, 0.14842379, 0.18914318, 0.1878766 ,\n",
       "       0.15661803, 0.15617885, 0.15144527, 0.1775958 , 0.19112673,\n",
       "       0.27345848, 0.14866912, 0.18131296, 0.17755084, 0.20847964,\n",
       "       0.18040925, 0.19256474, 0.19566947], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_max(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers printed above are the predicted labels for the first batch of training images. Let's compare them with the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
       "array([0, 8, 7, 1, 6, 7, 4, 3, 9, 4, 5, 4, 2, 1, 3, 8, 9, 4, 1, 9, 4, 7,\n",
       "       5, 0, 9, 1, 1, 0, 6, 4, 4, 4, 7, 3, 6, 4, 4, 1, 2, 7, 2, 9, 0, 3,\n",
       "       1, 6, 1, 2, 2, 7, 2, 3, 9, 6, 1, 8, 7, 4, 9, 0, 4, 3, 7, 9, 9, 8,\n",
       "       5, 3, 2, 4, 7, 8, 0, 4, 0, 6, 6, 7, 3, 9, 1, 1, 9, 4, 8, 1, 0, 4,\n",
       "       9, 6, 6, 3, 7, 5, 3, 8, 5, 1, 9, 1, 8, 1, 1, 7, 4, 3, 1, 1, 2, 6,\n",
       "       4, 8, 7, 1, 5, 9, 1, 2, 4, 3, 4, 9, 2, 2, 0, 4, 4, 2])>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metric and loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with linear regression, we need a way to evaluate how well our model is performing. A natural way to do this would be to find the percentage of labels that were predicted correctly i.e. the **accuracy** of the predictions. \n",
    "\n",
    "- We use `tf.reduce_sum` to get the sum all elements of a tensor\n",
    "- Since `preds==labels` will return a tensor with boolean values, we need to cast it to integers 0-1 to have the sum. It is done using `tf.cast`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    preds = tf.argmax(outputs, axis=1)\n",
    "    return tf.reduce_sum(tf.cast(preds == labels, dtype=tf.int32)) / len(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we don't need to apply softmax to the outputs, since it doesn't change the relative order of the results. This is because e^x is an increasing function i.e. if y1 > y2, then e^y1 > e^y2 and the same holds true after averaging out the values to get the softmax.\n",
    "\n",
    "Let's calculate the accuracy of the current model, on the first batch of data. Obviously, we expect it to be pretty bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.0625>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(outputs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the accuracy is a great way for us (humans) to evaluate the model, it can't be used as a loss function for optimizing our model using gradient descent, for the following reasons:\n",
    "\n",
    "1. It's not a differentiable function. `tf.argmax` and `==` are both non-continuous and non-differentiable operations, so we can't use the accuracy for computing gradients w.r.t the weights and biases.\n",
    "\n",
    "2. It doesn't take into account the actual probabilities predicted by the model, so it can't provide sufficient feedback for incremental improvements. \n",
    "\n",
    "Due to these reasons, accuracy is a great **evaluation metric** for classification, but not a good loss function. A commonly used loss function for classification problems is the **cross entropy**, which has the following formula:\n",
    "\n",
    "![cross-entropy](https://i.imgur.com/VDRDl1D.png)\n",
    "\n",
    "While it looks complicated, it's actually quite simple:\n",
    "\n",
    "* For each output row, pick the predicted probability for the correct label. E.g. if the predicted probabilities for an image are `[0.1, 0.3, 0.2, ...]` and the correct label is `1`, we pick the corresponding element `0.3` and ignore the rest.\n",
    "\n",
    "* Then, take the [logarithm](https://en.wikipedia.org/wiki/Logarithm) of the picked probability. If the probability is high i.e. close to 1, then its logarithm is a very small negative value, close to 0. And if the probability is low (close to 0), then the logarithm is a very large negative value. We also multiply the result by -1, which results is a large postive value of the loss for poor predictions.\n",
    "\n",
    "* Finally, take the average of the cross entropy across all the output rows to get the overall loss for a batch of data.\n",
    "\n",
    "Unlike accuracy, cross-entropy is a continuous and differentiable function that also provides good feedback for incremental improvements in the model (a slightly higher probability for the correct label leads to a lower loss). This makes it a good choice for the loss function. \n",
    "\n",
    "As you might expect, Tensorflow provides an efficient and tensor-friendly implementation of cross entropy as `tf.keras.losses.CategoricalCrossentropy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels hold a single value per image denoting which class the image belongs to. However, the loss categorical_crossentropy, expects the labels to be [one hot encoded](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f). Keras provides a helper `utils.to_categorical()` to convert the labels into one hot encoded vectors. \n",
    "\n",
    "One hot encoded vector has length equal to number of classes and has value 1 on actual class and 0 elsewhere. For exmaple, an image with label=7 will have 1 present at 7th index and rest all values will be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_categorical = tf.keras.utils.to_categorical(labels, num_classes=num_classes)\n",
    "label_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(14.637895, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Loss for current batch of data\n",
    "loss = loss_fn(tf.nn.softmax(outputs), label_categorical)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower the loss, the better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Now that we have defined the dataset, model, loss function and optimizer, we are ready to train the model. The training process is identical to linear regression, with the addition of a \"validation phase\" to evaluate the model in each epoch. Here's what it looks like in pseudocode:\n",
    "\n",
    "```\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    for batch in train_loader:\n",
    "        # Generate predictions\n",
    "        # Calculate loss\n",
    "        # Compute gradients\n",
    "        # Update weights\n",
    "    \n",
    "    # Validation phase\n",
    "    for batch in val_loader:\n",
    "        # Generate predictions\n",
    "        # Calculate loss\n",
    "        # Calculate metrics (accuracy etc.)\n",
    "    # Calculate average validation loss & metrics\n",
    "    \n",
    "    # Log epoch, loss & metrics for inspection\n",
    "```\n",
    "\n",
    "Some parts of the training loop are specific the specific problem we're solving (e.g. loss function, metrics etc.) whereas others are generic and can be applied to any deep learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some helper functions to perform validation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(model, x, y):\n",
    "    out = model(x)\n",
    "    out = tf.nn.softmax(out)\n",
    "    loss = loss_fn(out, tf.keras.utils.to_categorical(y, num_classes=num_classes))\n",
    "    acc = accuracy(out, y)\n",
    "    return {'loss': loss, 'acc': acc}\n",
    "\n",
    "def validation_epoch_end(outputs):\n",
    "    batch_losses = [x['loss'] for x in outputs]\n",
    "    epoch_loss = tf.reduce_mean(batch_losses)   # Combine losses\n",
    "    batch_accs = [x['acc'] for x in outputs]\n",
    "    epoch_acc =  tf.reduce_mean(batch_accs)      # Combine accuracies\n",
    "    return {'loss': epoch_loss, 'acc': epoch_acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define an `evaluate` function, which will perform the validation phase, and a `fit` function which will peform the entire training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_ds):\n",
    "    outputs = []\n",
    "    for x, y in val_ds:\n",
    "        outputs.append(validation_step(model, x, y))\n",
    "    return validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_ds, val_ds, opt_func=tf.keras.optimizers.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        for x, y in train_ds:\n",
    "            # Record all operations under GradientTape, so that gradients can be tracked and calculated later\n",
    "            with tf.GradientTape() as tape:\n",
    "                out = model(x)\n",
    "                out = tf.nn.softmax(out)\n",
    "                loss = loss_fn(out, tf.keras.utils.to_categorical(y, num_classes=10))\n",
    "            var_list_fn =  model.trainable_weights\n",
    "            # Compute gradients\n",
    "            grads = tape.gradient(loss, var_list_fn)\n",
    "#             print(grads)\n",
    "            # Apply gradients\n",
    "            optimizer.apply_gradients(zip(grads, var_list_fn))\n",
    "        # Validation phase\n",
    "        result = evaluate(model, train_ds)\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, train_acc: {:.4f}\".format(epoch, result['loss'], result['acc']))\n",
    "        result = evaluate(model, val_ds)\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['loss'], result['acc']))\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit` function records the validation loss and metric from each epoch and returns a history of the training process. This is useful for debuggin & visualizing the training process. Before we train the model, let's see how the model performs on the validation set with the initial set of randomly initialized weights & biases.\n",
    "\n",
    "Configurations like batch size, learning rate etc. need to picked in advance while training machine learning models, and are called hyperparameters. Picking the right hyperparameters is critical for training an accurate model within a reasonable amount of time, and is an active area of research and experimentation. Feel free to try different learning rates and see how it affects the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': <tf.Tensor: shape=(), dtype=float32, numpy=14.595188>,\n",
       " 'acc': <tf.Tensor: shape=(), dtype=float64, numpy=0.07199367088607594>}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result0 = evaluate(model, val_ds)\n",
    "result0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train the model. Let's train for 5 epochs and look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 12.1983, train_acc: 0.3573\n",
      "Epoch [0], val_loss: 12.2435, val_acc: 0.3528\n",
      "Epoch [1], train_loss: 9.8435, train_acc: 0.5760\n",
      "Epoch [1], val_loss: 9.8783, val_acc: 0.5789\n",
      "Epoch [2], train_loss: 7.9379, train_acc: 0.6345\n",
      "Epoch [2], val_loss: 7.9507, val_acc: 0.6372\n",
      "Epoch [3], train_loss: 7.2194, train_acc: 0.6435\n",
      "Epoch [3], val_loss: 7.2092, val_acc: 0.6489\n",
      "Epoch [4], train_loss: 6.7563, train_acc: 0.6627\n",
      "Epoch [4], val_loss: 6.7075, val_acc: 0.6694\n"
     ]
    }
   ],
   "source": [
    "history1 = fit(5, 0.001, model, train_ds, val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a great result! With just 5 epochs of training, our model has reached an accuracy of over 66% on the validation set. Let's see if we can improve that by training for a few more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 6.1452, train_acc: 0.7127\n",
      "Epoch [0], val_loss: 6.1445, val_acc: 0.7141\n",
      "Epoch [1], train_loss: 5.8445, train_acc: 0.7199\n",
      "Epoch [1], val_loss: 5.8713, val_acc: 0.7203\n",
      "Epoch [2], train_loss: 5.6321, train_acc: 0.7248\n",
      "Epoch [2], val_loss: 5.6090, val_acc: 0.7281\n",
      "Epoch [3], train_loss: 5.4704, train_acc: 0.7283\n",
      "Epoch [3], val_loss: 5.4409, val_acc: 0.7320\n",
      "Epoch [4], train_loss: 5.3484, train_acc: 0.7306\n",
      "Epoch [4], val_loss: 5.3578, val_acc: 0.7321\n",
      "Epoch [5], train_loss: 5.2425, train_acc: 0.7329\n",
      "Epoch [5], val_loss: 5.2265, val_acc: 0.7366\n",
      "Epoch [6], train_loss: 5.1591, train_acc: 0.7344\n",
      "Epoch [6], val_loss: 5.1148, val_acc: 0.7388\n",
      "Epoch [7], train_loss: 5.0860, train_acc: 0.7359\n",
      "Epoch [7], val_loss: 5.0646, val_acc: 0.7386\n",
      "Epoch [8], train_loss: 5.0225, train_acc: 0.7370\n",
      "Epoch [8], val_loss: 4.9764, val_acc: 0.7409\n",
      "Epoch [9], train_loss: 4.9588, train_acc: 0.7382\n",
      "Epoch [9], val_loss: 4.8989, val_acc: 0.7434\n",
      "Epoch [10], train_loss: 4.8876, train_acc: 0.7389\n",
      "Epoch [10], val_loss: 4.9003, val_acc: 0.7388\n",
      "Epoch [11], train_loss: 4.5975, train_acc: 0.7742\n",
      "Epoch [11], val_loss: 4.5406, val_acc: 0.7790\n",
      "Epoch [12], train_loss: 4.3285, train_acc: 0.8061\n",
      "Epoch [12], val_loss: 4.3243, val_acc: 0.8085\n",
      "Epoch [13], train_loss: 4.2042, train_acc: 0.8097\n",
      "Epoch [13], val_loss: 4.2011, val_acc: 0.8114\n",
      "Epoch [14], train_loss: 4.1086, train_acc: 0.8118\n",
      "Epoch [14], val_loss: 4.0814, val_acc: 0.8150\n"
     ]
    }
   ],
   "source": [
    "history2 = fit(15, 0.001, model, train_ds, val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the accuracy does continue to increase as we train for more epochs, the improvements get smaller with every epoch. This is easier to see using a line graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAgAElEQVR4nO3de3wV9Z3/8dcnN3IBwh2BcBPwgooCEZVatWpdrF21d6VWW22ttaxtbbfabX/Wtd1ttY9tt13dWrVWqyJeVi21qFW3tXW5CAgkgqCEiySAkISEEHLP5/fHTPAQknACmZwk5/18PM4jZ2a+M/M5c06+nzPf75nvmLsjIiLJKyXRAYiISGIpEYiIJDklAhGRJKdEICKS5JQIRESSnBKBiEiSUyIQ6SPM7MdmVmpmOxMdC4CZ3W5mjyY6Djk8JQJpk5n91cz2mFm/RMfSW5jZBDNzM1vUav6jZnZ7xPseB3wbmOrux0S5L+l7lAjkEGY2Afgw4MCl3bzvtO7cX0TOMLPZ3bzPcUCZu+/q5v1KH6BEIG25GlgKPARcE7vAzMaa2TNmttvMyszs7phlXzGzt82syszWmdmMcL6b2eSYcg+Z2Y/D5+eZWbGZ3RI2afzOzAab2fPhPvaEz/Ni1h9iZr8zs+3h8ufC+W+Z2T/GlEsPm0qmt36BYZwfj5lOC/c3w8wyw2/xZWZWYWbLzWxkJ47fXcC/tbcwPE4bzazczBaa2eh4NmpmuWb2+zDOrWb2AzNLMbMLgZeB0Wa2z8weamf9j5vZ6vA1LTazaTHLtpjZ98L3bU94fDPjidnMTjKzl8Nl75vZv8TsNiOMucrM1ppZfsx6t5hZSbhsg5ldEM9xkAi4ux56HPQANgI3AjOBBmBkOD8VWAP8AsgBMoGzw2WfAUqA0wEDJgPjw2UOTI7Z/kPAj8Pn5wGNwJ1APyALGAp8CsgGBgBPAc/FrP8n4AlgMJAOnBvO/y7wREy5y4DCdl7jbcBjMdOXAG+Hz78K/DHcf2p4HAbGcdwmhK91QHgsLgznPwrcHj4/HygFZoSv97+Av8X5vvwe+EO4/QnAO8B1McexuIN1pwO7gDPC13QNsAXoFy7fArwFjAWGAP8X8x61G3MYyw6CZqnMcPqMcNntQC3wsXCfPwGWhsuOB7YBo2OO3aREf/aT9ZHwAPToWQ/gbILKf1g4vR74Vvj8LGA3kNbGei8B32hnm4dLBPVAZgcxnQbsCZ+PApqBwW2UGw1UtVTawNPAd9vZ5uSwbHY4/RhwW/j8WmAxMK2Tx64lEaQRJNKWSi82EfwWuCtmnf7h8Z5wmG2nhsdpasy8rwJ/jTmOHSWCXwM/ajVvAx8k0S3ADTHLPgYUHS5m4EpgVTv7vB14JWZ6KlATc/x3ARcC6Yn+3Cf7Q01D0to1wJ/dvTScns8HzUNjga3u3tjGemOBoiPc5253r22ZMLNsM/tN2PyxF/gbMMjMUsP9lLv7ntYbcfftBN9kP2Vmg4CLCSr4Q7j7RuBt4B/NLJugL2R+uPgRgsS2IGx+usvM0jv5mh4ARsY2VYVGA1tj4tgHlAFjDrO9YQRnP1tj5m2NY70W44Fvh81CFWZWQXAsY5ultrXadsuyjmI+3Pse+wum/UCmmaWFx/+bBMlil5ktiLeJTLqeEoEcYGZZwGeBc81sZ9hm/y3gVDM7laCiGNdOh+42YFI7m95P0MzSovWvWloPgfttgqaDM9x9IHBOS4jhfoaEFX1bHgauImiqWuLuJe2UA3ic4BvtZcC6sHLC3Rvc/V/dfSowG/g4Qb9J3Ny9HvhX4Edh3C22E1TKwQsyyyFoCusoTgiaZhpi1yXoID7cei22Af/m7oNiHtnu/nhMmbGttr09jpi3AcfGGcNB3H2+u58dbtsJmgclAZQIJNblQBPBKfxp4eNE4O8EFeEbBO3BPzWznLBT9UPhug8A3zGzmRaYbGYtlcdqYK6ZpZrZHODcw8QxAKgBKsxsCPDDlgXuvgN4AfjvsFM53czOiVn3OYK27G8QtKl3ZAFwEfA1PjgbwMw+YmanhGcgewkq4ObDbKstjxC0m8+Jmfc48CUzO82Cn+b+O7DM3bd0tCF3bwKeBP7NzAaEx/ZmgmaneNwP3GBmZ4TvT46ZXWJmA2LKfN3M8sJj/n2CfpjDxfw8MMrMvmlm/cLYzjhcMGZ2vJmdH26vluD9PpJjLF0h0W1TevScB/Ai8B9tzP8swSl+GsE3xecImgZKgV/FlLuBoN15H0HH4/Rwfj6wlqBN/hGCiiW2j6C41f5GA38Nt/MOQVu4E/ZNEHRmPgy8D+wBnmm1/gNANdA/jtf8KkFn9TEx864MX0d1uI9fxez7XuDedrY1ITbOmGPnhH0EMcepCCgnqEjzwvnjwtc8rp3tDyao+HcTfBO/DUhp7zi2sf4cYDlQQZDQnwIGhMu2AN8D1oXLHybsP+ko5nDZyeFx3BN+Tm4N598OPNrW8QGmEXyxqIrZ5uhE/w8k68PCN0ikzzCz24Dj3P2qRMfSW5jZFuDL7v5KomOR7tcXLt4ROSBs1rgO+EKiYxHpLdRHIH2GmX2FoMnkBXf/W6LjEekt1DQkIpLkdEYgIpLkel0fwbBhw3zChAmJDkNEpFdZuXJlqbsPb2tZr0sEEyZMYMWKFYkOQ0SkVzGzre0tU9OQiEiSUyIQEUlySgQiIklOiUBEJMkpEYiIJDklAhGRHuze14pYXFR60LzFRaXc+9qR3v7jUEoEIiIROtqKfFpeLvPmrzqwjcVFpcybv4ppebldFqMSgYjIYRxNZR5vRd7U7FTub2Bb+X7Wbq9kSVEZL63dScmeGi45ZRTXPbSCW/5nDfPmr+LuudOZPWlYl72+XjfWUH5+vuuCMhHpTi2Vd0sFHDs9a8IQquua2FffSHVdI/vqGtlf18S+umC6ur6Rtdv38ofVJRw/cgBv76zi5NG5pKUae2saqKptDP7WtXUH2EPddP5kbr7o+E6/BjNb6e75bS2L9Mri8G5UvyS48fYD7v7TVsvHEdwAY1BY5lZ3XxRlTCIinTV70jDu/OQ0rn1oOf37pVFeXU9OvzS+9Lvl1DXGf2O1NcWVDOiXRk1DEwNT0xg7JJuBmekMzEoL/6YzIDPtoHm5Wems31HFLf9TwFVnjuPRZe9x5qShXXpGEFkiCG/zdw/wUaAYWG5mC919XUyxHwBPuvuvzWwqsIjgLkYiIj3CvrpGHnx9M/f/fRO1Dc3UNtRz/Mj+5E8YQv9+aWRnpJHTL5X+/dLI6ZcWzks98DynXxprSyq5+ak1XHVGUJH/v4+fGHdFvriolFueKeDuzwdnI2dOGtrlzUNRnhHMAja6+yYAM1tAeJPwmDIODAyf5/LBzbJFRBKqpr6JR5Zu4dd/LWLP/gZmjh/Mu7uq+OJZE3h02XtcMm1UXBXx4qJSbn5qzYGKu7MVeUFx5UFlZ08axt1zp1NQXNlliSCyPgIz+zQwx92/HE5/ATjD3efFlBkF/JngXqw5wIXuvrKNbV0PXA8wbty4mVu3tjt2kojIUalrbOKJ5du4+383squqjnOOG85HTxzBL155t80+gsNVxve+VsS0vNyDyi0uKqWguJIbzp0U9cs5oKM+gkQngpvDGP7DzM4Cfguc7O7tNrqps1hEotDY1Mwzb5bwy1ffpaSihlkThvDti47jjGOH9pjK/GgkqrO4BBgbM50Xzot1HTAHwN2XmFkmMAzYFWFcIiIHNDU7zxds5z9feZfNpdWcmpfLTz55Ch+eMgwzA2izsp89aViXdtgmUpSJYDkwxcwmEiSAK4C5rcq8B1wAPGRmJwKZwO4IYxKRJNX6W72786tXN/LI0i2U7qvnhGMGcP/V+Vx44ogDCSBZRJYI3L3RzOYBLxH8NPRBd19rZncAK9x9IfBt4H4z+xZBx/EXvbdd2CAivULLhV13XzmduqZm7vjjOjaXVjMqN5P/unI6l5wyipSU5EoALXRBmYgkjT+u2c63nlhNY7OTYnD9OZP4zkXHkZba9wdZSNgFZSIiPcXCNdv5wbOFB6a/du4k/nnOCQmMqOfo+2lQRJJaZU0D31ywipseX8WIgf3I6ZfGTedP5vHl2w4ZPyhZ6YxARPqsJUVlfPvJ1bxfVcenZ+Tx6vr3+fVVMyK7Qre30hmBiPQ5dY1N/OSFt5n7wFIy0lJ4+oazmDyyP/d8fkabV+gmO50RiEif8s77VXxzwWrW7djLlbPG8YNLTiSnXxrTxw0+pGxfuhbgaCgRiEif0NzsPLxkCz95YT0D+qXxwNX5XDh1ZKLD6hWUCESkW0UxXMP7e2v5zlNr+Pu7pZx/wgju/NQ0hg/o11Uh93nqIxCRTulpt158oXAH//Cff2P5lnJ+fPnJ/PaafCWBTlIiEEmgo6lUE7VuRxW5u1PX2ETF/nq2V9Swcdc+CosrWbapjL9s2MWiwh2U7Knh0lNH8+WHV3DdQ8u5/vcruemCyRwzMJOK/fU0N7d/kWts3FW1DXznqTV87bE3yUxP5U83fZirzhyfdMNDdAVdWSxJ72iaKo62maOjWyC27sQMKtlmauqbqGloYnFRKXf8cR03f/Q4Thg1kLXbK/nPV97lWxcexyl5uRgQ1IlBxWhGOM8oLK7gZy9t4JY5JzB19EBWbC3nl69s5LqzJzJ2SDY1DU3U1DdSU99MTUMTtQ1N1NQ3sT/8u7OyhvU7qxiUnU55dT0DM9Npanb2NzTR1EFFHo/UFGNwdjqDszMYkhM8BudkMDQng4r9DTyzqpgvfWgif1hdQnF5Df3SU7j/6nw+PGX4Ue23r0vIMNRRUSKQthxNhdyZyvhI121saqaipoE91fWUV9ezZ3895dUN7Nlfz1sllby6fhdjB2fxXvl+Jg3rT3paSlgZh5Vw+EjEv2tGagpZGalkpaeSlZFKZnoqWekp7Kqqo3hPDVNG9mfmuMFkZaSSnZFKdkYaWenB86xw+oPnqWSnp7F2eyX/8mwhl502hmfeLObGjwRnBOXh8SnfX0/5vuBv7DGLzTEDM4Muznu/MFO//ImDEoH0eZ39Zr2/vonKmgYq9jdQWdPAG5vLue9vRcwYP5iVW/fwqRl5TBiWQ+v/j9hJJ5jYUlrNs6u2c9rYQazatoczJg4hPTUlrLwaKK+up7Kmod3YczJSSTGjqq6RkQP7MXFYzoHKNDM9layMlKASTk8lMyOV7IMq5FT+VLiDP6zeziemj+HTM/NwD2IL/n7wmj0I+oNlDs+tLuH5gh18cvoYrp49IaiwD+w3lcy0lDbH4Wk5vi23XuzMRVlHmnibm529tQ2UVddz72tFPLWi+Ihv5J6MlAikVziab/UV++v545rt/PTFDUwfO4gVW8s5Z8pwsjJSqawJKvvKsNKvrGmg8SibL9qTYjByYOaBZo3BORkMyU4P/uZkfDA//DsoO50339tz1JVqd657NGdQ0HXNaUfympOZEoH0Ch1VMGdOHMrOvbVsLdvPe+XVbC3bz9by/bxXtp+tZdXsrW08ZHu5WekMyk4nNyt4DMxKZ1DWB9MtywZmpbOltJq7XtzAZ/PzeGplMXd9+lTOOHbIgW21dD/GdkS2PHtjcxk3P7mGK2eNY8Hybd3y7TiR6ybybl1Hm4SSmRKBdJsjrSTcnT37G3jxrR38+6L1zBg3iGWby5k6aiB7axvYtqeG+sYP7mCalmLkDc5i3NAcxg/JZvzQbPbXN/Hb1zfzufyxPL2ymLs/H/033ER+O05kJ3ei9Na4ewIlAuk2bVaMj63iR5efRN7gbHZU1rKzsoYde2vZWVkbTteyc2/tQRU9QHqKMXnkgAMV/bih2YwfksP4odmMys08qO06Ud9wVTFJb6FEIN3qTwXb+daTaxiSnc6uqjoAWjfJZ6SmcExuZvAYmMmo8PnemgZ++/pmPps/lmdWlXRbu7NIX6dEIN1mSVEZ//T4m1TsDzpkTzhmABecOIJjcrMYNTCo7EflZjIkJ+OQC3/U/isSnYTdoczM5gC/JLhn8QPu/tNWy38BfCSczAZGuPugKGOSaLg79/99E3e+uIERA4Kbf1xz1ngeXfYeH5oc3wiPBcWVB1X6scMEKxGIRCeyMwIzSwXeAT4KFAPLgSvdfV075f8JmO7u13a0XZ0R9Dz76hr57tNrWFS4k1kTBvPurn0Hxn3Xt3qRniFRZwSzgI3uvikMYgFwGdBmIgCuBH4YYTwSgY27qvjqIyvZXFrNv3zsBJqanW9+9Dh9qxfpRaJMBGOAbTHTxcAZbRU0s/HAROB/21l+PXA9wLhx47o2Sjliiwp38M9PrSEzPZVHv3xGuxW9bv4h0rP1lPsRXAE87e5NbS109/uA+yBoGurOwORQjU3N3Pnieu7/+2amjxvEf39+BqNysxIdlogcoSgTQQkwNmY6L5zXliuAr0cYi3SR3VV1zJv/Jss2l3P1WeP5wSVTyUjTaOYivVmUiWA5MMXMJhIkgCuAua0LmdkJwGBgSYSxSBdYuXUPNz62ksqaBn7+2VP55Iy8RIckIl0gskTg7o1mNg94ieDnow+6+1ozuwNY4e4Lw6JXAAu8t13QkETcnUeWbuVHz69jVG4Wz3xtFlNHD0x0WCLSRSLtI3D3RcCiVvNuazV9e5QxSOe0vkK3pr6Jr/x+Ba9vDO4F+4vPnkZudnqCoxSRrqTGXTlI7G0It5RWc9EvXuP1jaV8ZmYeD1ydryQg0gf1lF8NSQ8xe9Iw/uXiE7j2oeU0NzsNTc4tc47na+dNTnRoIhIRJQIBYFdVLX9cs4M/rC6hoLjywPxrzhqvJCDSxykRJLHqukZeWruTZ1eV8H8bS2l2OHnMQK46YxzPF+zg6nCsoH84+RhdECbShykR9EEdDcl83dkTef3dUp5dVcLL696npqGJvMFZ3HjeZC6fPppdVXXMm7+K/74qGCvozElDNVaQSB+nRNAHtXT4HhjOeWMpNzy6kjOPHcr9f9tEWXU9g7LT+eSMMXxi+hhmjh98YEjoV97epRFARZKM7kfQR/11wy5uenwVJxwzkBVby2l26JeWwoUnjuTy6WM497jhuiJYJIkk7H4E0j0am5p55/19FBRXsKa4ksKSCjbsrKKhyXljSzl5g7O46YIpzDn5GAZm6uefInIwJYIeqr12/jXbKvjo1GMoLKlgzbZKCoorWLt9L3Xh/X4HZqYxLW8QHzt5FK+u38Xn8vN4dvV28gZnKQmISJuUCHqolnb+n37yFBqanBfe2sGLb+0kPdW488UNAGSlpwa/8jlzPNPycpmWN4gJQ7NZsqmMefNXcd/VM5k9aRgXTB2pDl8RaZcSQQ/VcmHXVx9ZSUsvzqRhOcyePIxT8nI5NW8Qk0f0JzXFDllXt3wUkc5QIuihNpdWc9dLG8hMT6GmoZkbz5vEd+ecENe6N5w76ZB5ujmMiLRHPxvpgbaWVXPlfUupaWgiIy2Vm86fzILl21hcVJro0ESkD1Ii6GG2le9n7v3LqKprwAx+fdUMbr7oeO6eO/3AYHAiIl1JiaAHKamo4cr7l1JV28BnZo7l3qtmttnOLyLSldRH0EPsqKxh7v1Lqaxp4LEvn8G0vEGHlFE7v4hEQWcEPcD7e2uZe/8yyvbV8/trZ7WZBEREohJpIjCzOWa2wcw2mtmt7ZT5rJmtM7O1ZjY/ynh6ot1Vdcy9fym79tby8LWnM33c4ESHJCJJJrKmITNLBe4BPgoUA8vNbKG7r4spMwX4HvAhd99jZiOiiqcnKt0XJIHtFbU8fO0sZo4fkuiQRCQJRXlGMAvY6O6b3L0eWABc1qrMV4B73H0PgLvvijCeHqW8up6rHljGtj37efCLpzNropKAiCRGlIlgDLAtZro4nBfrOOA4M/s/M1tqZnPa2pCZXW9mK8xsxe7duyMKt/tU7A+SwObSan57zemcNWlookMSkSSW6M7iNGAKcB5wJXC/mR3SU+ru97l7vrvnDx8+vJtD7FqVNQ184bdvsHHXPu67Op8PTdavgEQksaJMBCXA2JjpvHBerGJgobs3uPtm4B2CxNAn7a1t4OoH32D9zr385gszOfe43p3URKRviDIRLAemmNlEM8sArgAWtirzHMHZAGY2jKCpaFOEMXWre18rOnAl8L66Rr744BsUFldw6amj+cgJSdUvLiI9WGSJwN0bgXnAS8DbwJPuvtbM7jCzS8NiLwFlZrYO+Avwz+5eFlVM3a1lKOn/Xf8+X/rdG6zeVkF2RhqfmpmX6NBERA7QrSojtnhjKVc/+AaNzU7/fmkH7hEgItKdOrpVZaI7i/u8EQP70dgcJNtrPzRBSUBEehwlgog9tvQ9AK4+azyPLntPo4eKSI+jRBChxUWlPLZsK0Nz0vnXS0/SUNIi0iMpEURozbYK+qWncu7xIzAzDSUtIj2SEkGEzj9hJFW1jZx17AdXDs+eNKzNW0mKiCSKEkGEloRNQGceqyEkRKTnUiKI0JJNZeQNzmLskOxEhyIi0i4lgog0NzvLNpcf1CwkItITKRFEZP3OKir2N2hkURHp8ZQIIrJkUzBShvoHRKSnUyKIyJKiMsYPzWb0oKxEhyIi0iElggg0NTvLNpepf0BEegUlggi8vWNvcP2A+gdEpBdQIojAkqKgf0BnBCLSGygRRGDJpjKOHZ7DiIGZiQ5FROSwlAi6WGNTM29sLtevhUSk14grEZjZM2Z2iZkpcRzG2u172VfXqGYhEek14q3Y/xuYC7xrZj81s+MjjKlX0/UDItLbxJUI3P0Vd/88MAPYArxiZovN7Etmlt7eemY2x8w2mNlGM7u1jeVfNLPdZrY6fHz5SF9IT7GkqIwpI/ozfEC/RIciIhKXuJt6zGwo8EXgy8Aq4JcEieHldsqnAvcAFwNTgSvNbGobRZ9w99PCxwOdC79naWhqZvkW9Q+ISO+SFk8hM3sWOB54BPhHd98RLnrCzNq7k/wsYKO7bwq3sQC4DFh3dCH3XAXFleyvb9L1AyLSq8SVCIBfuftf2lrg7vntrDMG2BYzXQyc0Ua5T5nZOcA7wLfcfVvrAmZ2PXA9wLhx4+IMufstVf+AiPRC8TYNTTWzQS0TZjbYzG7sgv3/EZjg7tMImpgebquQu9/n7vnunj98+PAu2G00lm4q44RjBjAkJyPRoYiIxC3eRPAVd69omXD3PcBXDrNOCTA2ZjovnHeAu5e5e104+QAwM854epz6xmZWbNmjswER6XXiTQSpZmYtE2FH8OG+9i4HppjZRDPLAK4AFsYWMLNRMZOXAm/HGU+Ps6a4gpqGJiUCEel14u0jeJGgY/g34fRXw3ntcvdGM5sHvASkAg+6+1ozuwNY4e4LgZvM7FKgESgn+FVSr7S0qAwzOPPYIYkORUSkU+JNBLcQVP5fC6dfJmjK6ZC7LwIWtZp3W8zz7wHfizOGHm3JpjJOPGYgg7LVPyAivUtcicDdm4Ffhw9ppa6xiZVb93DVmeMTHYqISKfFex3BFOAnBBeGHRhS092PjSiuXmXVexXUNTarf0BEeqV4O4t/R3A20Ah8BPg98GhUQfU2S4rKSDGYNVH9AyLS+8SbCLLc/VXA3H2ru98OXBJdWL3L0k1lnDQ6l9ysdoddEhHpseJNBHXhENTvmtk8M/sE0D/CuHqN2oYmVr1XoWElRKTXijcRfAPIBm4iuOjrKuCaqILqTd7cuof6pmb9bFREeq3DdhaHF499zt2/A+wDvhR5VL3Ikk1lpKYYp09QIhCR3umwZwTu3gSc3Q2x9EpLN5Vx8phcBmSqf0BEeqd4LyhbZWYLgaeA6paZ7v5MJFH1EjX1TazeVsF1Z+tXtCLSe8WbCDKBMuD8mHkOJHUiWLG1nIYmV0exiPRq8V5ZrH6BNiwpKiMtxcgfPzjRoYiIHLF4ryz+HcEZwEHc/douj6gXWbqpjGl5ueT0i/fESkSk54m3Bns+5nkm8Alge9eH03tU1zVSUFzJV89V/4CI9G7xNg39T+y0mT0OvB5JRL3E8i3lNDY7Zx07LNGhiIgclXgvKGttCjCiKwPpbZZsKiM91Zip/gER6eXi7SOo4uA+gp0E9yhIWkuLyjht7CCyMlITHYqIyFGJt2loQNSB9CZVtQ0UllQy7yOTEx2KiMhRi6tpyMw+YWa5MdODzOzyONabY2YbzGyjmd3aQblPmZmbWX58YSfW8i3lNDucqesHRKQPiLeP4IfuXtky4e4VwA87WiEco+ge4GKCG9pcaWZT2yg3gGBQu2XxBp1oS4rKyEhLYcY49Q+ISO8XbyJoq9zhmpVmARvdfZO71wMLgMvaKPcj4E6gNs5YEm7JpjKmjx1EZrr6B0Sk94s3Eawws5+b2aTw8XNg5WHWGQNsi5kuDucdYGYzgLHu/qe4I06wypoG1m7fq2ElRKTPiDcR/BNQDzxB8M2+Fvj60ew4vNHNz4Fvx1H2ejNbYWYrdu/efTS7PWpvbC7HHc7S/YlFpI+I91dD1UC7nb3tKAHGxkznhfNaDABOBv5qZgDHAAvN7FJ3X9Fq//cB9wHk5+cfMtRFd1pSVEa/tBROGzcokWGIiHSZeH819LKZDYqZHmxmLx1mteXAFDObaGYZwBXAwpaF7l7p7sPcfYK7TwCWAockgZ5myaYyZo4fTL809Q+ISN8Qb9PQsPCXQgC4+x4Oc2WxuzcC84CXgLeBJ919rZndYWaXHmnAibSnup63d+xVs5CI9CnxDjrXbGbj3P09ADObQBujkbbm7ouARa3m3dZO2fPijCVhlm0uB1BHsYj0KfEmgu8Dr5vZa4ABHwaujyyqHmrppjKy0lOZlqf+ARHpO+LtLH4xvOr3emAV8BxQE2VgPdGSojLyJwwmI+1Ix+oTEel54h107ssEV//mAauBM4ElHHzryj6tbF8dG96v4tLTRic6FBGRLhXvV9tvAKcDW939I8B0oKLjVfoW9Q+ISF8VbyKodfdaADPr5+7rgeOjC6vnuPe1IhYXlbKkqIycjFROGZPL4qJS7n2tKNGhiYh0iXg7i4vD6wieA142sz3A1ujC6jmm5eUyb/4qMtNTOH3iEJZvKWfe/FXcPXd6okMTEekS5t65C3XN7FwgF3gxHEyuW+Xn5/uKFd17zdmiwu3c+NgqZr6QqugAAA8ySURBVE8ayvqdVdw9dzqzJ+kWlSLSe5jZSndvc6j/eM8IDnD3144+pN4lxYIWtMVFZdx0/mQlARHpU/Q7yDi88NYOAG48bxKPLnuPxUWlCY5IRKTrKBEcxuKiUv5UuIMJQ7P57pwTuHvudObNX6VkICJ9hhLBYazZVkG/tJQDPxudPWkYd8+dTkFx5WHWFBHpHZQIDuOSU0ZTXdfEKWM+GFZi9qRh3HDupARGJSLSdZQIDqOgJLhublpeboIjERGJhhLBYRQWV5KRlsJxIwckOhQRkUgoERxGQXElJ44aqIHmRKTPUu3WgeZm562SSqaNUbOQiPRdSgQd2FJWTVVdI6eof0BE+jAlgg60/ERUHcUi0pdFmgjMbI6ZbTCzjWZ2axvLbzCzQjNbbWavm9nUKOPprILiSjLTU5g8vH+iQxERiUxkicDMUoF7gIuBqcCVbVT08939FHc/DbgL+HlU8RyJwpIKThqdS1qqTpxEpO+KsoabBWx0903hKKULgMtiC7j73pjJHKBzQ6FGqKnZeatkL6eoo1hE+rhOjz7aCWOAbTHTxcAZrQuZ2deBm4EM2rn1pZldT3C/ZMaNG9flgbalaPc+ahqaOHWsEoGI9G0Jb/Nw93vcfRJwC/CDdsrc5+757p4/fPjwbomrpaM4dmgJEZG+KMpEUAKMjZnOC+e1ZwFweYTxdEphcQU5GakcOywn0aGIiEQqykSwHJhiZhPNLAO4AlgYW8DMpsRMXgK8G2E8nbKmuJKTx+SSkmKJDkVEJFKRJQJ3bwTmAS8BbwNPuvtaM7vDzC4Ni80zs7Vmtpqgn+CaqOLpjIamZtbt2KvrB0QkKUTZWYy7LwIWtZp3W8zzb0S5/yP1zvtV1Dc2c0qe+gdEpO9LeGdxT1TYckWxfjoqIklAiaANBSWVDMxMY/zQ7ESHIiISOSWCNhQWVzItbxBm6igWkb5PiaCVusYm1u/cqxFHRSRpKBG0sn5HFQ1Nrv4BEUkaSgStFJSEVxTrjEBEkoQSQSuFxRUMyclgzKCsRIciItItlAhaKSiu5JQxueooFpGkoUQQo6a+iXd37dMVxSKSVJQIYqzbsZemZmearigWkSSiRBCjsLgC0D2KRSS5KBHEKCipZMSAfowcmJnoUEREuo0SQYyC4kqdDYhI0lEiCO2ra6Ro9z7dkUxEko4SQWhtSSXu6h8QkeSjRBAq1BXFIpKklAhCBcWVjBmUxbD+/RIdiohIt1IiCBWWBFcUi4gkm0gTgZnNMbMNZrbRzG5tY/nNZrbOzArM7FUzGx9lPO2prGlgc2m1moVEJClFlgjMLBW4B7gYmApcaWZTWxVbBeS7+zTgaeCuqOLpyFth/4A6ikUkGUV5RjAL2Ojum9y9HlgAXBZbwN3/4u77w8mlQF6E8bSrILxHsZqGRCQZRZkIxgDbYqaLw3ntuQ54oa0FZna9ma0wsxW7d+/uwhADhSUVjBuSzaDsjC7ftohIT9cjOovN7CogH/hZW8vd/T53z3f3/OHDh3f5/guKK9U/ICJJK8pEUAKMjZnOC+cdxMwuBL4PXOrudRHG06by6nqK99To1pQikrSiTATLgSlmNtHMMoArgIWxBcxsOvAbgiSwK8JY2lV4oKNYQ0uISHKKLBG4eyMwD3gJeBt40t3XmtkdZnZpWOxnQH/gKTNbbWYL29lcZFqGnj55zMDu3rWISI+QFuXG3X0RsKjVvNtinl8Y5f7jsaa4kmOH5zAgMz3RoYiIJESP6CxOpMLiSvUPiEhSS+pEsGtvLTv31nKK+gdEJIkldSIo1BXFIiLJnQgKiitJMThptDqKRSR5JXUiKCypZMqIAWRnRNpnLiLSoyVtInB3XVEsIkISJ4IdlbWU7qtT/4CIJL2kTQQacVREJJC0iaCwpIK0FOPEUeooFpHklrSJoKC4kuNGDiAzPTXRoYiIJFRSJgJ3p7CkUv0DIiIkaSIo3lNDxf4G/WJIRIQkTQQtHcWnamgJEZEkTQQlFWSkpnDcyAGJDkVEJOGSMxFsq+TEUQPISEvKly8icpCkqwmbm523SnRFsYhIi6RLBFvKqqmqa2TaGPUPiIhAxInAzOaY2QYz22hmt7ax/Bwze9PMGs3s01HG0qJl6GmdEYiIBCJLBGaWCtwDXAxMBa40s6mtir0HfBGYH1UcrRUUV5KZnsKUEf27a5ciIj1alOMvzwI2uvsmADNbAFwGrGsp4O5bwmXNEcZxkMLiSk4anUtaatK1iomItCnK2nAMsC1mujic12lmdr2ZrTCzFbt37z7igJqanbe2V2qgORGRGL3ia7G73+fu+e6eP3z48CPeTtHufeyvb9LQEiIiMaJMBCXA2JjpvHBewrRcUaxEICLygSgTwXJgiplNNLMM4ApgYYT7O6zC4gpyMlKZOEwdxSIiLSJLBO7eCMwDXgLeBp5097VmdoeZXQpgZqebWTHwGeA3ZrY2qngACkoqOWlMLqkpFuVuRER6lUjv2u7ui4BFrebdFvN8OUGTUWTufa2IaXm5nD5hCOu27+ULZ45ncVEpBcWV3HDupCh3LSLSK/SKzuKjMS0vl3nzV/H0imLqGpvJTE9h3vxV6icQEQn1+UQwe9Iw7p47nR//Kbh84ZGl73H33OnMnjQswZGJiPQMfT4RQJAMPnLCCACuPmu8koCISIykSASLi0pZXFTGTedP5rFl77G4qDTRIYmI9Bh9PhEsLipl3vxV3D13OjdfdDx3z53OvPmrlAxEREJ9PhEUFFce1CfQ0mfQcnGZiEiyM3dPdAydkp+f7ytWrEh0GCIivYqZrXT3/LaW9fkzAhER6ZgSgYhIklMiEBFJckoEIiJJTolARCTJ9bpfDZnZbmDrEa4+DOiJFxAors5RXJ3XU2NTXJ1zNHGNd/c27+zV6xLB0TCzFe39fCqRFFfnKK7O66mxKa7OiSouNQ2JiCQ5JQIRkSSXbIngvkQH0A7F1TmKq/N6amyKq3MiiSup+ghERORQyXZGICIirSgRiIgkuT6ZCMxsjpltMLONZnZrG8v7mdkT4fJlZjahG2Iaa2Z/MbN1ZrbWzL7RRpnzzKzSzFaHj9uijivc7xYzKwz3ecjQrhb4VXi8CsxsRjfEdHzMcVhtZnvN7JutynTb8TKzB81sl5m9FTNviJm9bGbvhn8Ht7PuNWGZd83smohj+pmZrQ/fp2fNbFA763b4nkcU2+1mVhLzfn2snXU7/P+NIK4nYmLaYmar21k3kmPWXt3QrZ8vd+9TDyAVKAKOBTKANcDUVmVuBO4Nn18BPNENcY0CZoTPBwDvtBHXecDzCThmW4BhHSz/GPACYMCZwLIEvKc7CS6IScjxAs4BZgBvxcy7C7g1fH4rcGcb6w0BNoV/B4fPB0cY00VAWvj8zrZiiuc9jyi224HvxPFed/j/29VxtVr+H8Bt3XnM2qsbuvPz1RfPCGYBG919k7vXAwuAy1qVuQx4OHz+NHCBmVmUQbn7Dnd/M3xeBbwNjIlyn13oMuD3HlgKDDKzUd24/wuAInc/0ivKj5q7/w0obzU79nP0MHB5G6v+A/Cyu5e7+x7gZWBOVDG5+5/dvTGcXArkdcW+Oqud4xWPeP5/I4krrAM+CzzeVfuLM6b26oZu+3z1xUQwBtgWM13MoRXugTLhP00lMLRbogPCpqjpwLI2Fp9lZmvM7AUzO6mbQnLgz2a20syub2N5PMc0SlfQ/j9nIo5Xi5HuviN8vhMY2UaZRB67awnO5NpyuPc8KvPCZqsH22nqSOTx+jDwvru/287yyI9Zq7qh2z5ffTER9Ghm1h/4H+Cb7r631eI3CZo/TgX+C3ium8I6291nABcDXzezc7ppv4dlZhnApcBTbSxO1PE6hAfn6T3mt9hm9n2gEXisnSKJeM9/DUwCTgN2EDTD9CRX0vHZQKTHrKO6IerPV19MBCXA2JjpvHBem2XMLA3IBcqiDszM0gne6Mfc/ZnWy919r7vvC58vAtLNbFjUcbl7Sfh3F/Aswel5rHiOaVQuBt509/dbL0jU8YrxfksTWfh3Vxtluv3YmdkXgY8Dnw8rkEPE8Z53OXd/392b3L0ZuL+dfSbksxbWA58EnmivTJTHrJ26ods+X30xESwHppjZxPDb5BXAwlZlFgItveufBv63vX+YrhK2P/4WeNvdf95OmWNa+irMbBbB+xNpgjKzHDMb0PKcoLPxrVbFFgJXW+BMoDLmlDVq7X5LS8TxaiX2c3QN8Ic2yrwEXGRmg8OmkIvCeZEwsznAd4FL3X1/O2Xiec+jiC22X+kT7ewznv/fKFwIrHf34rYWRnnMOqgbuu/z1dU94D3hQfArl3cIfn3w/XDeHQT/HACZBE0NG4E3gGO7IaazCU7tCoDV4eNjwA3ADWGZecBagl9KLAVmd0Ncx4b7WxPuu+V4xcZlwD3h8SwE8rvpfcwhqNhzY+Yl5HgRJKMdQANBO+x1BP1KrwLvAq8AQ8Ky+cADMeteG37WNgJfijimjQRtxi2fsZZfx40GFnX0nnfD8Xok/PwUEFRyo1rHFk4f8v8bZVzh/IdaPlcxZbvlmHVQN3Tb50tDTIiIJLm+2DQkIiKdoEQgIpLklAhERJKcEoGISJJTIhARSXJKBCLdyIIRU59PdBwisZQIRESSnBKBSBvM7CozeyMce/43ZpZqZvvM7BfhmPGvmtnwsOxpZrbUPrgHwOBw/mQzeyUcFO9NM5sUbr6/mT1twX0DHot65FuRw1EiEGnFzE4EPgd8yN1PA5qAzxNc6bzC3U8CXgN+GK7ye+AWd59GcOVsy/zHgHs8GBRvNsEVrRCMLvlNgjHnjwU+FPmLEulAWqIDEOmBLgBmAsvDL+tZBAN+NfPBoGSPAs+YWS4wyN1fC+c/DDwVjkszxt2fBXD3WoBwe294OKaNBXfDmgC8Hv3LEmmbEoHIoQx42N2/d9BMs//XqtyRjs9SF/O8Cf0fSoKpaUjkUK8CnzazEXDg3rHjCf5fPh2WmQu87u6VwB4z+3A4/wvAax7caarYzC4Pt9HPzLK79VWIxEnfRERacfd1ZvYDgrtRpRCMVPl1oBqYFS7bRdCPAMEQwfeGFf0m4Evh/C8AvzGzO8JtfKYbX4ZI3DT6qEiczGyfu/dPdBwiXU1NQyIiSU5nBCIiSU5nBCIiSU6JQEQkySkRiIgkOSUCEZEkp0QgIpLk/j87FDjC110iyAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Replace these values with your results\n",
    "history = [result0] + history1 + history2\n",
    "accuracies = [result['acc'] for result in history]\n",
    "plt.plot(accuracies, '-x')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Accuracy vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite clear from the above picture that the model probably won't cross the accuracy threshold of 90% even after training for a very long time. One possible reason for this is that the learning rate might be too high. It's possible that the model's paramaters are \"bouncing\" around the optimal set of parameters that have the lowest loss. You can try reducing the learning rate and training for a few more epochs to see if it helps.\n",
    "\n",
    "The more likely reason that **the model just isn't powerful enough**. If you remember our initial hypothesis, we have assumed that the output (in this case the class probabilities) is a **linear function** of the input (pixel intensities), obtained by perfoming a matrix multiplication with the weights matrix and adding the bias. This is a fairly weak assumption, as there may not actually exist a linear relationship between the pixel intensities in an image and the digit it represents. While it works reasonably well for a simple dataset like MNIST (getting us to 85% accuracy), we need more sophisticated models that can capture non-linear relationships between image pixels and labels for complex tasks like recognizing everyday objects, animals etc. \n",
    "\n",
    "This would be a good time to save our work. Along with the notebook, we can also record some metrics from our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8149723101265823"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history[-1]['acc'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.081425"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history[-1]['loss'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type 'float32' is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-663e68d190a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjovian\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/envs/tftwo/lib/python3.6/site-packages/jovian/utils/records.py\u001b[0m in \u001b[0;36mlog_metrics\u001b[0;34m(data_dict, verbose, **data_args)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mjovian\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m \u001b[0;34m'epochs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mlog_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'metrics'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tftwo/lib/python3.6/site-packages/jovian/utils/records.py\u001b[0m in \u001b[0;36mlog_record\u001b[0;34m(record_type, data, verbose, **data_args)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# Send to API endpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mtracking_slug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tracking'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trackingSlug'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# Save to data block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tftwo/lib/python3.6/site-packages/jovian/utils/api.py\u001b[0m in \u001b[0;36mpost_block\u001b[0;34m(data, data_type, version)\u001b[0m\n\u001b[1;32m    110\u001b[0m                \u001b[0;34m\"data\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                \"recordType\": data_type}]\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpost_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tftwo/lib/python3.6/site-packages/jovian/utils/api.py\u001b[0m in \u001b[0;36mpost_blocks\u001b[0;34m(blocks, version)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpost_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_u\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/data/record'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_h\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tftwo/lib/python3.6/site-packages/jovian/utils/request.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_request_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m401\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The current API key is invalid or expired.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tftwo/lib/python3.6/site-packages/jovian/utils/request.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;34m\"\"\"Retryable POST request\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tftwo/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tftwo/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tftwo/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         )\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mprep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tftwo/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerge_setting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mcookies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerged_cookies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerge_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m         )\n\u001b[1;32m    461\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tftwo/lib/python3.6/site-packages/requests/models.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_cookies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tftwo/lib/python3.6/site-packages/requests/models.py\u001b[0m in \u001b[0;36mprepare_body\u001b[0;34m(self, data, files, json)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0;31m# provides this natively, but Python 3 gives a Unicode string.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0mcontent_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'application/json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m                 \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         default is None and not sort_keys and not kw):\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m/usr/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m    179\u001b[0m         raise TypeError(\"Object of type '%s' is not JSON serializable\" %\n\u001b[0;32m--> 180\u001b[0;31m                         o.__class__.__name__)\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type 'float32' is not JSON serializable"
     ]
    }
   ],
   "source": [
    "jovian.log_metrics(val_acc=history[-1]['acc'].numpy(), val_loss=history[-1]['loss'].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading the model\n",
    "Since we've trained our model for a long time and achieved a resonable accuracy, it would be a good idea to save the weights and bias matrices to disk, so that we can reuse the model later and avoid retraining from scratch. Here's how you can save the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's evaluate the current mode's accuracy and loss on validation set for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.0535293>,\n",
       " 'acc': <tf.Tensor: shape=(), dtype=float64, numpy=0.8762856012658228>}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result0 = evaluate(model, val_ds)\n",
    "result0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights for the parameters in model can be saved by using `model.save_weights()` function similar to state_dict() in torch. Note that in order to load the model again, the model definition has to be same (like we defined using Sequential api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train for a couple more epochs to make the model shift weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 2.6336, train_acc: 0.8879\n",
      "Epoch [0], val_loss: 2.6016, val_acc: 0.8882\n",
      "Epoch [1], train_loss: 2.4319, train_acc: 0.8935\n",
      "Epoch [1], val_loss: 2.4085, val_acc: 0.8926\n"
     ]
    }
   ],
   "source": [
    "history3 = fit(2, 0.01, model, train_ds, val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the current model's performance on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.4060795>,\n",
       " 'acc': <tf.Tensor: shape=(), dtype=float64, numpy=0.8927017405063291>}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result0 = evaluate(model, val_ds)\n",
    "result0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If for whatevever reason, we need to reload the older weights again, it can be done using `model.load_weights()` api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.0632868>,\n",
       " 'acc': <tf.Tensor: shape=(), dtype=float64, numpy=0.8753955696202531>}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result0 = evaluate(model, val_ds)\n",
    "result0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing against random images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = ds_test.map(lambda x,y: (mapp(x),y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1\n",
      "Predicted 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for x, y in val_ds:\n",
    "    plt.imshow(x[0].numpy().reshape(28,28), cmap='gray')\n",
    "    print(\"Label:\", y[0].numpy())\n",
    "    pred = model(x)\n",
    "#     print(pred[0])\n",
    "    print(\"Predicted\", tf.argmax(pred[0]).numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commit and upload the notebook\n",
    "\n",
    "As a final step, we can save and commit our work using the jovian library. Along with the notebook, we can also attach the weights of our trained model, so that we can use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "jovian.commit(filename='03-tf-logistic-regression', outputs=['model.h5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
